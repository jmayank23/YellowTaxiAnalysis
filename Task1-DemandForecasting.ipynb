{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We obtained our dataset from New York City Gov [website](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page). For this project, we decided to use the Yellow Taxi Trip Records data for the year 2023. The data was available in parquet files for each month. All the data was donwloaded and can be found inside `/home/mjain3/uci150/mjain3/yellow_taxi_data`\n",
    "\n",
    "Additionally, we also used New York shape data (from the same gov website mentioned above) to be able to do visualizations on the map using geopandas. The link to the shapefile is [here](https://d37ci6vzurychx.cloudfront.net/misc/taxi_zones.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T02:24:36.302374Z",
     "iopub.status.busy": "2024-06-05T02:24:36.301310Z",
     "iopub.status.idle": "2024-06-05T02:24:37.039366Z",
     "shell.execute_reply": "2024-06-05T02:24:37.038861Z",
     "shell.execute_reply.started": "2024-06-05T02:24:36.302357Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mj/conda_env/tf/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import dayofmonth, month, hour, minute, year, col, floor, count, isnull, udf\n",
    "from pyspark.sql.types import LongType, DecimalType, IntegerType, StructType, StructField, StringType, BooleanType\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/05 07:10:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "\t.appName(\"Yellow Taxi Task 1\") \\\n",
    "\t.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T02:24:37.040687Z",
     "iopub.status.busy": "2024-06-05T02:24:37.040430Z",
     "iopub.status.idle": "2024-06-05T02:24:39.434586Z",
     "shell.execute_reply": "2024-06-05T02:24:39.434044Z",
     "shell.execute_reply.started": "2024-06-05T02:24:37.040671Z"
    }
   },
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#     .config(\"spark.driver.memory\", \"4g\") \\\n",
    "# \t.config(\"spark.executor.memory\", \"2g\") \\\n",
    "#     .config('spark.executor.instances', 6) \\\n",
    "# \t.appName(\"Yellow Taxi Task 1\") \\\n",
    "# \t.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T02:24:39.435462Z",
     "iopub.status.busy": "2024-06-05T02:24:39.435319Z",
     "iopub.status.idle": "2024-06-05T02:24:39.967841Z",
     "shell.execute_reply": "2024-06-05T02:24:39.967368Z",
     "shell.execute_reply.started": "2024-06-05T02:24:39.435445Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mayanks-mbp-4.lan1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Yellow Taxi Task 1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x138e644f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T02:24:39.969184Z",
     "iopub.status.busy": "2024-06-05T02:24:39.968986Z",
     "iopub.status.idle": "2024-06-05T02:24:39.971997Z",
     "shell.execute_reply": "2024-06-05T02:24:39.971601Z",
     "shell.execute_reply.started": "2024-06-05T02:24:39.969170Z"
    }
   },
   "outputs": [],
   "source": [
    "base_path = 'yellow_taxi_data/yellow_tripdata_2023-{:02d}.parquet'\n",
    "\n",
    "paths=[]\n",
    "for mo in range(1, 2):  # This loops from 1 to 12\n",
    "    path = base_path.format(mo)  # Formats the month with leading zero if necessary\n",
    "    paths.append(path)\n",
    "\n",
    "# paths = ['yellow_taxi_data/yellow_tripdata_2023-01.parquet', 'yellow_taxi_data/yellow_tripdata_2023-02.parquet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T02:24:39.972592Z",
     "iopub.status.busy": "2024-06-05T02:24:39.972462Z",
     "iopub.status.idle": "2024-06-05T02:24:43.359664Z",
     "shell.execute_reply": "2024-06-05T02:24:43.359221Z",
     "shell.execute_reply.started": "2024-06-05T02:24:39.972580Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2023-01-01 00:32:10|  2023-01-01 00:40:36|            1.0|         0.97|       1.0|                 N|         161|         141|           2|        9.3|  1.0|    0.5|       0.0|         0.0|                  1.0|        14.3|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to load and cast a single Parquet file\n",
    "def load_and_cast(filepath):\n",
    "    df = spark.read.parquet(filepath)\n",
    "    df = df.withColumn(\"VendorID\", col(\"VendorID\").cast(IntegerType()))\n",
    "    return df\n",
    "\n",
    "# Load, cast, and accumulate all DataFrames\n",
    "dataframes = [load_and_cast(path) for path in paths]\n",
    "df = reduce(lambda df1, df2: df1.unionByName(df2), dataframes)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T02:24:43.360294Z",
     "iopub.status.busy": "2024-06-05T02:24:43.360157Z",
     "iopub.status.idle": "2024-06-05T02:24:43.727497Z",
     "shell.execute_reply": "2024-06-05T02:24:43.727108Z",
     "shell.execute_reply.started": "2024-06-05T02:24:43.360280Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3066766"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T02:24:43.728074Z",
     "iopub.status.busy": "2024-06-05T02:24:43.727943Z",
     "iopub.status.idle": "2024-06-05T02:24:43.732435Z",
     "shell.execute_reply": "2024-06-05T02:24:43.731933Z",
     "shell.execute_reply.started": "2024-06-05T02:24:43.728060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T02:24:43.733188Z",
     "iopub.status.busy": "2024-06-05T02:24:43.733040Z",
     "iopub.status.idle": "2024-06-05T02:24:43.853917Z",
     "shell.execute_reply": "2024-06-05T02:24:43.853302Z",
     "shell.execute_reply.started": "2024-06-05T02:24:43.733174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "- Extraction of Time Components: extracted day, month, year, and time from the tpep_pickup_datetime column to separate columns (pickup_day, pickup_month, pickup_year).\n",
    "- Labeling Holidays: used the USFederalHolidayCalendar to identify if a particular day is a holiday or not (is_holiday)\n",
    "- 15-Minute, 30-Minute, and 60-Minute Indexing: calculated a 15/30/60min index for each record, representing which block of the day the pickup occurred. This was done by dividing the total minutes past midnight by 15/30/60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T02:24:43.855576Z",
     "iopub.status.busy": "2024-06-05T02:24:43.855383Z",
     "iopub.status.idle": "2024-06-05T02:24:44.118662Z",
     "shell.execute_reply": "2024-06-05T02:24:44.118278Z",
     "shell.execute_reply.started": "2024-06-05T02:24:43.855563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- pickup_year: integer (nullable = true)\n",
      " |-- pickup_day: integer (nullable = true)\n",
      " |-- pickup_month: integer (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      "\n",
      "+--------------------+------------+-----------+----------+------------+-----------+\n",
      "|tpep_pickup_datetime|PULocationID|pickup_year|pickup_day|pickup_month|pickup_time|\n",
      "+--------------------+------------+-----------+----------+------------+-----------+\n",
      "| 2023-01-01 00:32:10|         161|       2023|         1|           1|         32|\n",
      "| 2023-01-01 00:55:08|          43|       2023|         1|           1|         55|\n",
      "| 2023-01-01 00:25:04|          48|       2023|         1|           1|         25|\n",
      "| 2023-01-01 00:03:48|         138|       2023|         1|           1|          3|\n",
      "| 2023-01-01 00:10:29|         107|       2023|         1|           1|         10|\n",
      "| 2023-01-01 00:50:34|         161|       2023|         1|           1|         50|\n",
      "| 2023-01-01 00:09:22|         239|       2023|         1|           1|          9|\n",
      "| 2023-01-01 00:27:12|         142|       2023|         1|           1|         27|\n",
      "| 2023-01-01 00:21:44|         164|       2023|         1|           1|         21|\n",
      "| 2023-01-01 00:39:42|         141|       2023|         1|           1|         39|\n",
      "| 2023-01-01 00:53:01|         234|       2023|         1|           1|         53|\n",
      "| 2023-01-01 00:43:37|          79|       2023|         1|           1|         43|\n",
      "| 2023-01-01 00:34:44|         164|       2023|         1|           1|         34|\n",
      "| 2023-01-01 00:09:29|         138|       2023|         1|           1|          9|\n",
      "| 2023-01-01 00:33:53|          33|       2023|         1|           1|         33|\n",
      "| 2023-01-01 00:13:04|          79|       2023|         1|           1|         13|\n",
      "| 2023-01-01 00:45:11|          90|       2023|         1|           1|         45|\n",
      "| 2023-01-01 00:04:33|         113|       2023|         1|           1|          4|\n",
      "| 2023-01-01 00:03:36|         237|       2023|         1|           1|          3|\n",
      "| 2023-01-01 00:15:23|         143|       2023|         1|           1|         15|\n",
      "+--------------------+------------+-----------+----------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter selected features for the task: tpep_pickup_datetime and PULocationID \n",
    "df_task1 = df[\"tpep_pickup_datetime\",\"PULocationID\"]\n",
    "\n",
    "# Add pickup: day, month, time columns to df\n",
    "df_task1 = df_task1.withColumn(\"pickup_year\", year(\"tpep_pickup_datetime\"))\n",
    "df_task1 = df_task1.withColumn(\"pickup_day\", dayofmonth(\"tpep_pickup_datetime\"))\n",
    "df_task1 = df_task1.withColumn(\"pickup_month\", month(\"tpep_pickup_datetime\"))\n",
    "df_task1 = df_task1.withColumn(\"pickup_time\", hour(\"tpep_pickup_datetime\") * 60 + minute(\"tpep_pickup_datetime\"))\n",
    "\n",
    "# Show the modified DataFrame structure and some rows to verify\n",
    "df_task1.printSchema()\n",
    "df_task1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T02:24:44.119243Z",
     "iopub.status.busy": "2024-06-05T02:24:44.119113Z",
     "iopub.status.idle": "2024-06-05T02:24:45.487975Z",
     "shell.execute_reply": "2024-06-05T02:24:45.487528Z",
     "shell.execute_reply.started": "2024-06-05T02:24:44.119229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- pickup_year: integer (nullable = true)\n",
      " |-- pickup_day: integer (nullable = true)\n",
      " |-- pickup_month: integer (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- is_holiday: boolean (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----------+----------+------------+-----------+----------+\n",
      "|tpep_pickup_datetime|PULocationID|pickup_year|pickup_day|pickup_month|pickup_time|is_holiday|\n",
      "+--------------------+------------+-----------+----------+------------+-----------+----------+\n",
      "| 2023-01-01 00:32:10|         161|       2023|         1|           1|         32|     false|\n",
      "| 2023-01-01 00:55:08|          43|       2023|         1|           1|         55|     false|\n",
      "| 2023-01-01 00:25:04|          48|       2023|         1|           1|         25|     false|\n",
      "| 2023-01-01 00:03:48|         138|       2023|         1|           1|          3|     false|\n",
      "| 2023-01-01 00:10:29|         107|       2023|         1|           1|         10|     false|\n",
      "| 2023-01-01 00:50:34|         161|       2023|         1|           1|         50|     false|\n",
      "| 2023-01-01 00:09:22|         239|       2023|         1|           1|          9|     false|\n",
      "| 2023-01-01 00:27:12|         142|       2023|         1|           1|         27|     false|\n",
      "| 2023-01-01 00:21:44|         164|       2023|         1|           1|         21|     false|\n",
      "| 2023-01-01 00:39:42|         141|       2023|         1|           1|         39|     false|\n",
      "| 2023-01-01 00:53:01|         234|       2023|         1|           1|         53|     false|\n",
      "| 2023-01-01 00:43:37|          79|       2023|         1|           1|         43|     false|\n",
      "| 2023-01-01 00:34:44|         164|       2023|         1|           1|         34|     false|\n",
      "| 2023-01-01 00:09:29|         138|       2023|         1|           1|          9|     false|\n",
      "| 2023-01-01 00:33:53|          33|       2023|         1|           1|         33|     false|\n",
      "| 2023-01-01 00:13:04|          79|       2023|         1|           1|         13|     false|\n",
      "| 2023-01-01 00:45:11|          90|       2023|         1|           1|         45|     false|\n",
      "| 2023-01-01 00:04:33|         113|       2023|         1|           1|          4|     false|\n",
      "| 2023-01-01 00:03:36|         237|       2023|         1|           1|          3|     false|\n",
      "| 2023-01-01 00:15:23|         143|       2023|         1|           1|         15|     false|\n",
      "+--------------------+------------+-----------+----------+------------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to determine if a date is a holiday\n",
    "def is_holiday(date):\n",
    "    year, month, day = date.year, date.month, date.day\n",
    "    cal = USFederalHolidayCalendar()\n",
    "    holidays = cal.holidays(start=f'{year}-01-01', end=f'{year}-12-31').to_pydatetime()\n",
    "    return pd.Timestamp(year, month, day) in holidays\n",
    "\n",
    "# Register UDF\n",
    "is_holiday_udf = udf(is_holiday, BooleanType())\n",
    "\n",
    "# Apply UDF to create a new column 'is_holiday'\n",
    "df_task1 = df_task1.withColumn(\"is_holiday\", is_holiday_udf(col(\"tpep_pickup_datetime\")))\n",
    "\n",
    "# Show the modified DataFrame structure and some rows to verify\n",
    "df_task1.printSchema()\n",
    "df_task1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T02:24:45.488621Z",
     "iopub.status.busy": "2024-06-05T02:24:45.488485Z",
     "iopub.status.idle": "2024-06-05T02:24:46.067616Z",
     "shell.execute_reply": "2024-06-05T02:24:46.067058Z",
     "shell.execute_reply.started": "2024-06-05T02:24:45.488607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- pickup_year: integer (nullable = true)\n",
      " |-- pickup_day: integer (nullable = true)\n",
      " |-- pickup_month: integer (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- is_holiday: boolean (nullable = true)\n",
      " |-- 15min_index: long (nullable = true)\n",
      " |-- 30min_index: long (nullable = true)\n",
      " |-- 60min_index: long (nullable = true)\n",
      "\n",
      "+--------------------+------------+-----------+----------+------------+-----------+----------+-----------+-----------+-----------+\n",
      "|tpep_pickup_datetime|PULocationID|pickup_year|pickup_day|pickup_month|pickup_time|is_holiday|15min_index|30min_index|60min_index|\n",
      "+--------------------+------------+-----------+----------+------------+-----------+----------+-----------+-----------+-----------+\n",
      "| 2023-01-01 00:32:10|         161|       2023|         1|           1|         32|     false|          2|          1|          0|\n",
      "| 2023-01-01 00:55:08|          43|       2023|         1|           1|         55|     false|          3|          1|          0|\n",
      "| 2023-01-01 00:25:04|          48|       2023|         1|           1|         25|     false|          1|          0|          0|\n",
      "| 2023-01-01 00:03:48|         138|       2023|         1|           1|          3|     false|          0|          0|          0|\n",
      "| 2023-01-01 00:10:29|         107|       2023|         1|           1|         10|     false|          0|          0|          0|\n",
      "| 2023-01-01 00:50:34|         161|       2023|         1|           1|         50|     false|          3|          1|          0|\n",
      "| 2023-01-01 00:09:22|         239|       2023|         1|           1|          9|     false|          0|          0|          0|\n",
      "| 2023-01-01 00:27:12|         142|       2023|         1|           1|         27|     false|          1|          0|          0|\n",
      "| 2023-01-01 00:21:44|         164|       2023|         1|           1|         21|     false|          1|          0|          0|\n",
      "| 2023-01-01 00:39:42|         141|       2023|         1|           1|         39|     false|          2|          1|          0|\n",
      "| 2023-01-01 00:53:01|         234|       2023|         1|           1|         53|     false|          3|          1|          0|\n",
      "| 2023-01-01 00:43:37|          79|       2023|         1|           1|         43|     false|          2|          1|          0|\n",
      "| 2023-01-01 00:34:44|         164|       2023|         1|           1|         34|     false|          2|          1|          0|\n",
      "| 2023-01-01 00:09:29|         138|       2023|         1|           1|          9|     false|          0|          0|          0|\n",
      "| 2023-01-01 00:33:53|          33|       2023|         1|           1|         33|     false|          2|          1|          0|\n",
      "| 2023-01-01 00:13:04|          79|       2023|         1|           1|         13|     false|          0|          0|          0|\n",
      "| 2023-01-01 00:45:11|          90|       2023|         1|           1|         45|     false|          3|          1|          0|\n",
      "| 2023-01-01 00:04:33|         113|       2023|         1|           1|          4|     false|          0|          0|          0|\n",
      "| 2023-01-01 00:03:36|         237|       2023|         1|           1|          3|     false|          0|          0|          0|\n",
      "| 2023-01-01 00:15:23|         143|       2023|         1|           1|         15|     false|          1|          0|          0|\n",
      "+--------------------+------------+-----------+----------+------------+-----------+----------+-----------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create 15 min index\n",
    "df_task1 = df_task1.withColumn(\"15min_index\", floor((hour(col(\"tpep_pickup_datetime\")) * 60 + minute(col(\"tpep_pickup_datetime\"))) / 15))\n",
    "df_task1 = df_task1.withColumn(\"30min_index\", floor((hour(col(\"tpep_pickup_datetime\")) * 60 + minute(col(\"tpep_pickup_datetime\"))) / 30))\n",
    "df_task1 = df_task1.withColumn(\"60min_index\", floor((hour(col(\"tpep_pickup_datetime\")) * 60 + minute(col(\"tpep_pickup_datetime\"))) / 60))\n",
    "\n",
    "# Show the modified DataFrame structure and some rows to verify\n",
    "df_task1.printSchema()\n",
    "df_task1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T02:24:46.068393Z",
     "iopub.status.busy": "2024-06-05T02:24:46.068257Z",
     "iopub.status.idle": "2024-06-05T02:24:49.086099Z",
     "shell.execute_reply": "2024-06-05T02:24:49.085691Z",
     "shell.execute_reply.started": "2024-06-05T02:24:46.068380Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=30047Kb max_used=30058Kb free=101024Kb\n",
      " bounds [0x00000001069e0000, 0x0000000108770000, 0x000000010e9e0000]\n",
      " total_blobs=11531 nmethods=10526 adapters=917\n",
      " compilation: disabled (not enough contiguous free space left)\n",
      "+--------------------+------------+-----------+----------+------------+-----------+----------+-----------+-----------+-----------+---------------+---------------+---------------+\n",
      "|tpep_pickup_datetime|PULocationID|pickup_year|pickup_day|pickup_month|pickup_time|is_holiday|60min_index|30min_index|15min_index|num_taxis_60min|num_taxis_30min|num_taxis_15min|\n",
      "+--------------------+------------+-----------+----------+------------+-----------+----------+-----------+-----------+-----------+---------------+---------------+---------------+\n",
      "| 2023-01-01 00:32:10|         161|       2023|         1|           1|         32|     false|          0|          1|          2|           1816|            750|            427|\n",
      "| 2023-01-01 00:55:08|          43|       2023|         1|           1|         55|     false|          0|          1|          3|            358|            146|             71|\n",
      "| 2023-01-01 00:25:04|          48|       2023|         1|           1|         25|     false|          0|          0|          1|           3807|           1941|            957|\n",
      "| 2023-01-01 00:03:48|         138|       2023|         1|           1|          3|     false|          0|          0|          0|           3208|           2096|           1198|\n",
      "| 2023-01-01 00:10:29|         107|       2023|         1|           1|         10|     false|          0|          0|          0|           2367|           1212|            595|\n",
      "| 2023-01-01 00:50:34|         161|       2023|         1|           1|         50|     false|          0|          1|          3|           1816|            750|            323|\n",
      "| 2023-01-01 00:09:22|         239|       2023|         1|           1|          9|     false|          0|          0|          0|           1221|            732|            376|\n",
      "| 2023-01-01 00:27:12|         142|       2023|         1|           1|         27|     false|          0|          0|          1|           1662|           1077|            474|\n",
      "| 2023-01-01 00:21:44|         164|       2023|         1|           1|         21|     false|          0|          0|          1|           2239|           1164|            536|\n",
      "| 2023-01-01 00:39:42|         141|       2023|         1|           1|         39|     false|          0|          1|          2|           1521|            721|            379|\n",
      "| 2023-01-01 00:53:01|         234|       2023|         1|           1|         53|     false|          0|          1|          3|           2552|           1124|            552|\n",
      "| 2023-01-01 00:43:37|          79|       2023|         1|           1|         43|     false|          0|          1|          2|           6754|           3416|           1711|\n",
      "| 2023-01-01 00:34:44|         164|       2023|         1|           1|         34|     false|          0|          1|          2|           2239|           1075|            550|\n",
      "| 2023-01-01 00:09:29|         138|       2023|         1|           1|          9|     false|          0|          0|          0|           3208|           2096|           1198|\n",
      "| 2023-01-01 00:33:53|          33|       2023|         1|           1|         33|     false|          0|          1|          2|             38|             23|             11|\n",
      "| 2023-01-01 00:13:04|          79|       2023|         1|           1|         13|     false|          0|          0|          0|           6754|           3338|           1658|\n",
      "| 2023-01-01 00:45:11|          90|       2023|         1|           1|         45|     false|          0|          1|          3|           1898|            869|            403|\n",
      "| 2023-01-01 00:04:33|         113|       2023|         1|           1|          4|     false|          0|          0|          0|           1227|            694|            351|\n",
      "| 2023-01-01 00:03:36|         237|       2023|         1|           1|          3|     false|          0|          0|          0|           1592|            930|            520|\n",
      "| 2023-01-01 00:15:23|         143|       2023|         1|           1|         15|     false|          0|          0|          1|            343|            190|             90|\n",
      "+--------------------+------------+-----------+----------+------------+-----------+----------+-----------+-----------+-----------+---------------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by PULocationID and 60min_index, then count the occurrences\n",
    "df_with_num_taxis_60min = df_task1.groupBy(\"PULocationID\", \"60min_index\") \\\n",
    "                                  .agg(count(\"*\").alias(\"num_taxis_60min\"))\n",
    "\n",
    "# Group by PULocationID and 30min_index, then count the occurrences\n",
    "df_with_num_taxis_30min = df_task1.groupBy(\"PULocationID\", \"30min_index\") \\\n",
    "                                  .agg(count(\"*\").alias(\"num_taxis_30min\"))\n",
    "\n",
    "# Group by PULocationID and 15min_index, then count the occurrences\n",
    "df_with_num_taxis_15min = df_task1.groupBy(\"PULocationID\", \"15min_index\") \\\n",
    "                                  .agg(count(\"*\").alias(\"num_taxis_15min\"))\n",
    "\n",
    "# Join the original DataFrame with the counts for 60min_index\n",
    "df_final = df_task1.join(df_with_num_taxis_60min, on=[\"PULocationID\", \"60min_index\"], how=\"left\")\n",
    "\n",
    "# Join the resulting DataFrame with the counts for 30min_index\n",
    "df_final = df_final.join(df_with_num_taxis_30min, on=[\"PULocationID\", \"30min_index\"], how=\"left\")\n",
    "\n",
    "# Join the resulting DataFrame with the counts for 15min_index\n",
    "df_final = df_final.join(df_with_num_taxis_15min, on=[\"PULocationID\", \"15min_index\"], how=\"left\")\n",
    "\n",
    "# Select the desired columns, including the 60min_index, 30min_index, and 15min_index\n",
    "df_final = df_final.select(\"tpep_pickup_datetime\", \"PULocationID\", \"pickup_year\", \"pickup_day\", \n",
    "                           \"pickup_month\", \"pickup_time\", \"is_holiday\", \"60min_index\", \n",
    "                           \"30min_index\", \"15min_index\", \"num_taxis_60min\", \n",
    "                           \"num_taxis_30min\", \"num_taxis_15min\")\n",
    "\n",
    "# Show the result\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T02:24:49.086815Z",
     "iopub.status.busy": "2024-06-05T02:24:49.086674Z",
     "iopub.status.idle": "2024-06-05T02:24:49.169309Z",
     "shell.execute_reply": "2024-06-05T02:24:49.168926Z",
     "shell.execute_reply.started": "2024-06-05T02:24:49.086801Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[tpep_pickup_datetime: timestamp_ntz, PULocationID: bigint, pickup_year: int, pickup_day: int, pickup_month: int, pickup_time: int, is_holiday: boolean, 60min_index: bigint, 30min_index: bigint, 15min_index: bigint, num_taxis_60min: bigint, num_taxis_30min: bigint, num_taxis_15min: bigint]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization to confirm pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T02:24:49.169961Z",
     "iopub.status.busy": "2024-06-05T02:24:49.169828Z",
     "iopub.status.idle": "2024-06-05T02:24:49.173215Z",
     "shell.execute_reply": "2024-06-05T02:24:49.172766Z",
     "shell.execute_reply.started": "2024-06-05T02:24:49.169947Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Convert to pd (For histogram Visualization)\n",
    "# hist_data_15min = df_final.groupBy(\"15min_index\").count().orderBy(\"15min_index\")\n",
    "# hist_data_30min = df_final.groupBy(\"30min_index\").count().orderBy(\"30min_index\")\n",
    "# hist_data_60min = df_final.groupBy(\"60min_index\").count().orderBy(\"60min_index\")\n",
    "\n",
    "# # hist_data_15min = df_task1.groupBy(\"15min_index\").count().orderBy(\"15min_index\")\n",
    "# # hist_data_30min = df_task1.groupBy(\"30min_index\").count().orderBy(\"30min_index\")\n",
    "# # hist_data_60min = df_task1.groupBy(\"60min_index\").count().orderBy(\"60min_index\")\n",
    "\n",
    "# hist_pd_15min = hist_data_15min.toPandas()\n",
    "# hist_pd_30min = hist_data_30min.toPandas()\n",
    "# hist_pd_60min = hist_data_60min.toPandas()\n",
    "\n",
    "# # Create a figure and a set of subplots\n",
    "# fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\n",
    "\n",
    "# # Plotting on the first axis for 15min_index\n",
    "# ax[0,0].bar(hist_pd_15min['15min_index'], hist_pd_15min['count'])\n",
    "# ax[0,0].set_xlabel('15 Minute Index')\n",
    "# ax[0,0].set_ylabel('Count')\n",
    "# ax[0,0].set_title('Histogram of 15min_index')\n",
    "# ax[0,0].set_xticks(range(0, 96, 5))\n",
    "\n",
    "# # Plotting on the second axis for 30min_index\n",
    "# ax[0,1].bar(hist_pd_30min['30min_index'], hist_pd_30min['count'], color='red')\n",
    "# ax[0,1].set_xlabel('30 Minute Index')\n",
    "# ax[0,1].set_ylabel('Count')\n",
    "# ax[0,1].set_title('Histogram of 30min_index')\n",
    "# ax[0,1].set_xticks(range(0, 48, 2))\n",
    "\n",
    "# # Plotting on the second axis for 60min_index\n",
    "# ax[1,0].bar(hist_pd_60min['60min_index'], hist_pd_60min['count'], color='green')\n",
    "# ax[1,0].set_xlabel('60 Minute Index')\n",
    "# ax[1,0].set_ylabel('Count')\n",
    "# ax[1,0].set_title('Histogram of 60min_index')\n",
    "# ax[1,0].set_xticks(range(0, 24, 1))\n",
    "\n",
    "# # Display the plot\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the ground truth features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T02:24:49.174018Z",
     "iopub.status.busy": "2024-06-05T02:24:49.173820Z",
     "iopub.status.idle": "2024-06-05T02:24:49.212753Z",
     "shell.execute_reply": "2024-06-05T02:24:49.212248Z",
     "shell.execute_reply.started": "2024-06-05T02:24:49.174002Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create ground truth columns for 15min, 30min, and 60min slots\n",
    "# df_15min = df_task1.groupBy(\"pickup_year\", \"pickup_month\", \"pickup_day\", \"PULocationID\", \"is_holiday\", \"15min_index\").agg(count(\"*\").alias(\"num_taxis_15min\"))\n",
    "# df_30min = df_task1.groupBy(\"pickup_year\", \"pickup_month\", \"pickup_day\", \"PULocationID\", \"is_holiday\", \"30min_index\").agg(count(\"*\").alias(\"num_taxis_30min\"))\n",
    "# df_60min = df_task1.groupBy(\"pickup_year\", \"pickup_month\", \"pickup_day\", \"PULocationID\", \"is_holiday\", \"60min_index\").agg(count(\"*\").alias(\"num_taxis_60min\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T02:21:28.396846Z",
     "iopub.status.busy": "2024-06-05T02:21:28.396702Z",
     "iopub.status.idle": "2024-06-05T02:21:28.427066Z",
     "shell.execute_reply": "2024-06-05T02:21:28.426759Z",
     "shell.execute_reply.started": "2024-06-05T02:21:28.396834Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_60min.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T02:21:28.427794Z",
     "iopub.status.busy": "2024-06-05T02:21:28.427509Z",
     "iopub.status.idle": "2024-06-05T02:21:28.491577Z",
     "shell.execute_reply": "2024-06-05T02:21:28.491269Z",
     "shell.execute_reply.started": "2024-06-05T02:21:28.427781Z"
    }
   },
   "outputs": [],
   "source": [
    "# Join original DataFrame with the aggregated data\n",
    "# df_15min = df_task1.join(df_15min, on=[\"pickup_year\", \"pickup_month\", \"pickup_day\", \"PULocationID\", \"is_holiday\", \"15min_index\"], how=\"left\")\n",
    "# df_30min = df_task1.join(df_30min, on=[\"pickup_year\", \"pickup_month\", \"pickup_day\", \"PULocationID\", \"is_holiday\", \"30min_index\"], how=\"left\")\n",
    "# df_60min = df_task1.join(df_60min, on=[\"pickup_year\", \"pickup_month\", \"pickup_day\", \"PULocationID\", \"is_holiday\", \"60min_index\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T02:24:49.690038Z",
     "iopub.status.busy": "2024-06-05T02:24:49.689708Z",
     "iopub.status.idle": "2024-06-05T02:24:49.800994Z",
     "shell.execute_reply": "2024-06-05T02:24:49.800495Z",
     "shell.execute_reply.started": "2024-06-05T02:24:49.690022Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature_cols = [\"pickup_year\", \"pickup_month\", \"pickup_day\", \"PULocationID\", \"is_holiday\", \"pickup_time\"]\n",
    "feature_cols = [\"60min_index\", \"pickup_month\", \"pickup_day\", \"PULocationID\", \"is_holiday\", \"pickup_time\"]\n",
    "\n",
    "# assembler_15min = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "# assembler_30min = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "assembler_60min = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# df_15min = assembler_15min.transform(df_15min)\n",
    "# df_30min = assembler_30min.transform(df_30min)\n",
    "df_60min = assembler_60min.transform(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T02:21:28.690096Z",
     "iopub.status.busy": "2024-06-05T02:21:28.689781Z",
     "iopub.status.idle": "2024-06-05T02:21:28.691859Z",
     "shell.execute_reply": "2024-06-05T02:21:28.691541Z",
     "shell.execute_reply.started": "2024-06-05T02:21:28.690082Z"
    }
   },
   "outputs": [],
   "source": [
    "# Summary statistics for num_taxis_15min\n",
    "# df_15min.describe().show()\n",
    "\n",
    "# Summary statistics for num_taxis_30min\n",
    "# df_30min.describe().show()\n",
    "\n",
    "# Summary statistics for num_taxis_60min\n",
    "# df_60min.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T02:24:52.181416Z",
     "iopub.status.busy": "2024-06-05T02:24:52.181119Z",
     "iopub.status.idle": "2024-06-05T02:24:52.198885Z",
     "shell.execute_reply": "2024-06-05T02:24:52.198473Z",
     "shell.execute_reply.started": "2024-06-05T02:24:52.181399Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "seed = 42\n",
    "\n",
    "# train_15min, test_15min = df_15min.randomSplit([0.8, 0.2], seed=seed)\n",
    "# train_30min, test_30min = df_30min.randomSplit([0.8, 0.2], seed=seed)\n",
    "train_60min, test_60min = df_60min.randomSplit([0.8, 0.2], seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T02:24:53.816990Z",
     "iopub.status.busy": "2024-06-05T02:24:53.816675Z",
     "iopub.status.idle": "2024-06-05T02:24:53.823312Z",
     "shell.execute_reply": "2024-06-05T02:24:53.822816Z",
     "shell.execute_reply.started": "2024-06-05T02:24:53.816974Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"num_taxis_15min\", metricName=\"rmse\")\n",
    "# evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"num_taxis_30min\", metricName=\"rmse\")\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"num_taxis_60min\", metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.spark import SparkXGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T02:25:00.044329Z",
     "iopub.status.busy": "2024-06-05T02:25:00.043998Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 09:48:31,216 INFO XGBoost-PySpark: _fit Running xgboost-2.0.3 on 1 workers with\n",
      "\tbooster params: {'objective': 'reg:squarederror', 'device': 'cpu', 'silent': False, 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "/Users/mj/conda_env/tf/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[09:48:39] task 0 got new rank 0                                    (0 + 1) / 1]\n",
      "/Users/mj/conda_env/tf/lib/python3.9/site-packages/xgboost/core.py:160: UserWarning: [09:48:41] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "2024-06-05 09:48:50,542 INFO XGBoost-PySpark: _fit Finished xgboost training!   \n"
     ]
    }
   ],
   "source": [
    "xgboost_regressor = SparkXGBRegressor(features_col=\"features\", label_col=\"num_taxis_60min\", silent=False)\n",
    "xgb_regression_model = xgboost_regressor.fit(train_60min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbr_predictions = xgb_regression_model.transform(test_60min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[tpep_pickup_datetime: timestamp_ntz, PULocationID: bigint, pickup_year: int, pickup_day: int, pickup_month: int, pickup_time: int, is_holiday: boolean, 60min_index: bigint, 30min_index: bigint, 15min_index: bigint, num_taxis_60min: bigint, num_taxis_30min: bigint, num_taxis_15min: bigint, features: vector, prediction: double]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbr_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:XGBoost-PySpark:Do the inference on the CPUs\n",
      "/Users/mj/conda_env/tf/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/Users/mj/conda_env/tf/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/Users/mj/conda_env/tf/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/Users/mj/conda_env/tf/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/Users/mj/conda_env/tf/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/Users/mj/conda_env/tf/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/Users/mj/conda_env/tf/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2024-06-05 09:48:52,079 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "2024-06-05 09:48:52,081 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "2024-06-05 09:48:52,084 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "2024-06-05 09:48:52,096 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "2024-06-05 09:48:52,099 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "2024-06-05 09:48:52,104 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "2024-06-05 09:48:52,146 INFO XGBoost-PySpark: predict_udf Do the inference on the CPUs\n",
      "[Stage 36:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for 60min model: 96.65698779126816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "xgb_rmse_60min = evaluator.evaluate(xgbr_predictions)\n",
    "print(f\"RMSE for 60min model: {xgb_rmse_60min}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkXGBRegressorModel' object has no attribute 'getFeatureImportances'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gg/n4bxt_9x6vd4py2h1f87z7bh0000gn/T/ipykernel_55662/4063294572.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get feature importances for from xgboost.spark import SparkXGBRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeature_importances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_regression_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetFeatureImportances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_importances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkXGBRegressorModel' object has no attribute 'getFeatureImportances'"
     ]
    }
   ],
   "source": [
    "# get feature importances for from xgboost.spark import SparkXGBRegressor\n",
    "feature_importances = xgb_regression_model.getFea\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T02:21:28.765875Z",
     "iopub.status.busy": "2024-06-05T02:21:28.765672Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 10:01:43 WARN Instrumentation: [4ea08fd3] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/06/05 10:01:47 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/06/05 10:01:49 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define Linear Regression models\n",
    "# lr_15min = LinearRegression(featuresCol=\"features\", labelCol=\"num_taxis_15min\")\n",
    "# lr_30min = LinearRegression(featuresCol=\"features\", labelCol=\"num_taxis_30min\")\n",
    "lr_60min = LinearRegression(featuresCol=\"features\", labelCol=\"num_taxis_60min\")\n",
    "\n",
    "# Train the models\n",
    "# lr_model_15min = lr_15min.fit(train_15min)\n",
    "# lr_model_30min = lr_30min.fit(train_30min)\n",
    "lr_model_60min = lr_60min.fit(train_60min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "# lr_predictions_15min = model_lr_15min.transform(test_15min)\n",
    "# lr_predictions_30min = model_lr_30min.transform(test_30min)\n",
    "lr_predictions_60min = lr_model_60min.transform(test_60min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for LR 60min model: 2817.1247046985304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Evaluate models\n",
    "# lr_rmse_15min = evaluator.evaluate(lr_predictions_15min)\n",
    "# print(f\"RMSE for 15min model: {lr_rmse_15min}\")\n",
    "\n",
    "\n",
    "# lr_rmse_30min = evaluator.evaluate(lr_predictions_30min)\n",
    "# print(f\"RMSE for 30min model: {lr_rmse_30min}\")\n",
    "\n",
    "\n",
    "lr_rmse_60min = evaluator.evaluate(lr_predictions_60min)\n",
    "print(f\"RMSE for LR 60min model: {lr_rmse_60min}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
